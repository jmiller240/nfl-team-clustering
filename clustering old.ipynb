{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e758db3",
   "metadata": {},
   "source": [
    "Start with\n",
    "\n",
    "- Formations (under center, shotgun, pistol)\n",
    "- Personnel (% 11, % mult TEs, % no TEs, % mult RBs, % no RBs, % extra OL)\n",
    "\n",
    "- % Pass\n",
    "- % Pass neutral downs\n",
    "- QB Scrambles\n",
    "\n",
    "- ADOT\n",
    "- % Screens\n",
    "- % Long\n",
    "- % passes from play-action\n",
    "- % passes from under center vs shotgun vs pistol\n",
    "- number receivers in top 80% targets\n",
    "\n",
    "- % runs middle, guard/tackle, edge\n",
    "- % rushes from under center vs. shotgun vs. pistol\n",
    "- number rushers to account for 20% rushes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Imports '''\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.stats.mstats import trimmed_var\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "from prep_data import load_pbp_participation_data, load_stats_team_tendencies_offense, load_stats_team_tendencies_defense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd88c4",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f27136",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_tendencies = load_stats_team_tendencies_offense()\n",
    "\n",
    "print(offense_tendencies.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df43d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Features '''\n",
    "# '% Pass Neutral Downs', '% Under Center Neutral Downs', '% Shotgun Neutral Downs',\n",
    "\n",
    "OFFENSE_FEATURES = [\n",
    "    'Plays / Game', 'Drives / Game', \n",
    "    '% Pass',  'Scrambles / Game',\n",
    "    '% Plays 11 Personnel', '% Plays Mult RBs', '% Plays Zero RBs', '% Plays Mult TEs', '% Plays Zero TEs', '% Plays Extra OL',\n",
    "    '% Under Center', '% Shotgun', 'Shotgun % Pass', 'Under Center % Pass',\n",
    "    'ADOT', 'ADOT to Sticks', 'Avg Time to Throw', '% Passes Behind LOS', '% Passes Deep', 'MaxTargetShare',\n",
    "    '% Rush Inside', '% Rush Outside', 'MaxRushAttemptsShare',\n",
    "]\n",
    "\n",
    "VIZ_FEATURES = ['Plays / Game', '% Pass', 'Scrambles / Game', \n",
    "                '% Plays Plays_11_Personnel',\n",
    "                '% Under Center', 'ADOT', 'Avg Time to Throw', 'MaxTargetShare', \n",
    "                '% Rush Outside', 'MaxRushAttemptsShare']\n",
    "\n",
    "# VIZ_FEATURES = ['Plays / Game', '% Pass', 'Scrambles / Game', \n",
    "#                 '% Plays Plays_11_Personnel', '% Plays Plays_Mult_RBs', '% Plays Plays_Mult_TEs',\n",
    "#                 '% Under Center', 'Shotgun % Pass', 'Under Center % Pass', \n",
    "#                 'ADOT', 'Avg Time to Throw', '% Passes Behind LOS', '% Passes Deep', 'MaxTargetShare', \n",
    "#                 '% Rush Outside', 'MaxRushAttemptsShare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580620c2",
   "metadata": {},
   "source": [
    "# Preprocessing & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Transform and Scale '''\n",
    "\n",
    "# ## Log transform data\n",
    "# transformed_data = pd.DataFrame(np.log(offense_tendencies[OFFENSE_FEATURES]), columns=OFFENSE_FEATURES).replace(math.inf, 0).replace(-(math.inf), 0)\n",
    "\n",
    "## Scale data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(offense_tendencies[OFFENSE_FEATURES])\n",
    "\n",
    "# Put `scaled_data` into DataFrame\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=OFFENSE_FEATURES)\n",
    "\n",
    "print(\"scaled DF type:\", type(scaled_data_df))\n",
    "print(\"scaled DF shape:\", scaled_data_df.shape)\n",
    "scaled_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ba265",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' PCA '''\n",
    "\n",
    "# Set number of PCA components to use after initial try\n",
    "PCA_N_COMPONENTS = 8\n",
    "COMPONENT_COLS = [f'Component {n}' for n in range(1, PCA_N_COMPONENTS + 1)]\n",
    "\n",
    "# Instantiate transformer\n",
    "pca_final = PCA(n_components=PCA_N_COMPONENTS, random_state=42)\n",
    "\n",
    "# Transform sku profiles\n",
    "pca_component_data_final = pca_final.fit_transform(scaled_data_df)\n",
    "\n",
    "# Evaluate components\n",
    "total_variance = scaled_data_df.var().sum()\n",
    "expl_variance = pca_final.explained_variance_.sum()\n",
    "\n",
    "print(f'Data set variance: {total_variance:,.3f}')\n",
    "print(f'PCA explained variance: {expl_variance:,.3f} ({round((expl_variance / total_variance) * 100, 2)}%)')\n",
    "\n",
    "pcs = pd.DataFrame(pca_final.components_, columns=OFFENSE_FEATURES)\n",
    "\n",
    "# Create bar charts of contribution\n",
    "for n in range(2):      #PCA_N_COMPONENTS):\n",
    "    pc = pcs.transpose()[n].sort_values(ascending=False)\n",
    "\n",
    "    fig = px.bar(\n",
    "        x=pc,\n",
    "        y=pc.index,\n",
    "        title=f\"PC{n+1}: Greatest contributors\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Correlation\", \n",
    "        yaxis_title=\"Features\", \n",
    "        yaxis={'dtick': 1, 'categoryorder':'total ascending'},\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    comp_expl_variance = pca_final.explained_variance_[n]\n",
    "    print(f'Explained variance: {comp_expl_variance:,} ({round((comp_expl_variance / total_variance) * 100, 2)}%)')\n",
    "                                                                                                                                                                                                              \n",
    "# Make df of PCA scores\n",
    "pca_component_df = pd.DataFrame(data=pca_component_data_final, columns=[f'Component {i}' for i in range(1, PCA_N_COMPONENTS + 1)])\n",
    "print(pca_component_df.shape)\n",
    "print(pca_component_df.head().to_string())\n",
    "\n",
    "# Add PCA scores to original dataframe\n",
    "offense_tendencies = offense_tendencies.drop(columns=list(filter(lambda x: x.startswith(\"Component\"), offense_tendencies.columns)))\n",
    "offense_tendencies = offense_tendencies.reset_index().merge(pca_component_df, left_index=True, right_index=True, how='left').set_index(['posteam', 'season'])\n",
    "\n",
    "\n",
    "print(f'PCA values')\n",
    "print(offense_tendencies.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' t-SNE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9afb0f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96965fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' KMeans Input '''\n",
    "\n",
    "kmeans_input = scaled_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e108506",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Kmeans Clustering '''\n",
    "\n",
    "# Try kmeans clustering with up to 20 clusters, keep track of inertia (basically cluster variance)\n",
    "n_clusters = range(2,10)\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for i in n_clusters:\n",
    "    # Model\n",
    "    kmeans = KMeans(n_clusters=i, n_init='auto', init='k-means++', random_state=42)\n",
    "\n",
    "    # Fit\n",
    "    kmeans.fit(kmeans_input)\n",
    "\n",
    "    # Score\n",
    "    ss = silhouette_score(kmeans_input, kmeans.labels_)   #, sample_size=int(len(pca_component_df) * 0.25))\n",
    "\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(ss)\n",
    "\n",
    "# Create scatter of inertia\n",
    "fig = px.line(\n",
    "    x=[i + 1 for i in range(len(inertia_values))],\n",
    "    y=inertia_values,\n",
    "    title=\"Kmeans - Inertia by Number of Clusters\"\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Num Clusters\", yaxis_title=\"Inertia\")\n",
    "fig.show()\n",
    "\n",
    "# Create a line plot of `silhouette_scores` vs `n_clusters`\n",
    "fig = px.line(\n",
    "    x=n_clusters,\n",
    "    y=silhouette_scores,\n",
    "    title=\"K-Means Model: Silhouette Score vs Number of Clusters\"\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Num Clusters\", yaxis_title=\"Silhouette Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Clustering - Final '''\n",
    "\n",
    "N_CLUSTERS = 5\n",
    "\n",
    "# Once optimal num clusters is found, create the final cluster model\n",
    "kmeans_final = KMeans(n_clusters=N_CLUSTERS, n_init='auto', init='k-means++', random_state=42)\n",
    "kmeans_final.fit(kmeans_input)\n",
    "\n",
    "# Find distances to centroids\n",
    "labels = kmeans_final.labels_\n",
    "distances_array = kmeans_final.transform(kmeans_input)\n",
    "\n",
    "distances_to_centroid = []\n",
    "for i in range(len(kmeans_input)):\n",
    "    # Get sku cluster / distances\n",
    "    label = labels[i]\n",
    "    centroid_distances = distances_array[i]\n",
    "\n",
    "    # Get distance to cluster center\n",
    "    distance_to_cluster_centroid = centroid_distances[label]\n",
    "    \n",
    "    # Append\n",
    "    distances_to_centroid.append(distance_to_cluster_centroid)\n",
    "\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "offense_tendencies['Cluster KMEANS'] = labels + 1\n",
    "offense_tendencies['Cluster KMEANS'] = offense_tendencies['Cluster KMEANS'].astype(str)\n",
    "offense_tendencies['Distance to KMEANS Centroid'] = distances_to_centroid\n",
    "\n",
    "print(offense_tendencies['Cluster KMEANS'].value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdef78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualize KMeans - 3D PCA '''\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=offense_tendencies,\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    z='Component 3',\n",
    "    title='KMeans Clusters',\n",
    "    color='Cluster KMEANS',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualize Clusters '''\n",
    "## Spider Chart each cluster\n",
    "\n",
    "\n",
    "def visualize_cluster(cluster: int, alg_name: str):\n",
    "\n",
    "    features = VIZ_FEATURES\n",
    "    cluster_col = f'Cluster {alg_name}'\n",
    "\n",
    "    ## Data ##\n",
    "\n",
    "    # Cluster feature averages\n",
    "    agg_dict = {feature: 'mean' for feature in OFFENSE_FEATURES}    # + COMPONENT_PERCENTILES}\n",
    "    agg_dict[cluster_col] = 'size'\n",
    "\n",
    "    avgs_by_cluster = offense_tendencies.groupby(cluster_col).aggregate(agg_dict)#.sort_values(by='SKU', ascending=False)\n",
    "    avgs_by_cluster = avgs_by_cluster.rename(columns={cluster_col: '# Teams'})\n",
    "    print(avgs_by_cluster.head().to_string())\n",
    "\n",
    "    # Get slice from offensive tendencies\n",
    "    cluster_sl = avgs_by_cluster.loc[avgs_by_cluster.index.get_level_values(cluster_col) == str(cluster), :]\n",
    "    n_teams = cluster_sl['# Teams'].values[0]\n",
    "\n",
    "    # PCA Component %iles\n",
    "    # cluster_component_pct_ranks = cluster_sl[COMPONENT_PERCENTILES].values.tolist()[0]\n",
    "\n",
    "    # Feature values\n",
    "    cluster_avg_vals = cluster_sl[features].values.tolist()[0]\n",
    "    \n",
    "    # Feature value percentiles\n",
    "    vals_fmt = []\n",
    "    pct_scores = []\n",
    "    pct_scores_fmt = []\n",
    "    for i in range(len(features)):\n",
    "        feature = features[i]\n",
    "\n",
    "        val = cluster_avg_vals[i]\n",
    "        pct_score = percentileofscore(offense_tendencies[feature].tolist(), val, kind='weak') / 100\n",
    "\n",
    "        val_fmt = f'{val:.1%}' if feature[0] == '%' else f'{val:.2f}'\n",
    "        vals_fmt.append(val_fmt)\n",
    "        pct_scores.append(pct_score)\n",
    "        pct_scores_fmt.append(f'{pct_score:.1%}')\n",
    "\n",
    "    ## Figure ##\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2, \n",
    "        column_widths=[4,3],\n",
    "        horizontal_spacing=0.1,\n",
    "        specs=[[{\"type\": \"polar\"}, {\"type\": \"domain\"}]]\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=pct_scores,     # cluster_component_pct_ranks,\n",
    "            theta=features,     # COMPONENT_NAMES,\n",
    "            opacity=0.7,\n",
    "            fill='toself'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Cluster {cluster}: {n_teams} teams\",\n",
    "        polar=dict(radialaxis_range=(0,1)),\n",
    "        margin=dict(b=50, r=50, l=75, t=75)\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            columnwidth=[2,1,1],\n",
    "            header={\n",
    "                \"values\": ['Feature', 'Value', 'Percentile'],\n",
    "            },\n",
    "            cells={\n",
    "                \"values\": [features, vals_fmt, pct_scores_fmt]\n",
    "            }\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "for cluster in range(1, N_CLUSTERS+1):\n",
    "    visualize_cluster(cluster=cluster, alg_name='KMEANS')\n",
    "\n",
    "    cluster_teams = offense_tendencies.loc[offense_tendencies['Cluster KMEANS'] == str(cluster),:]\n",
    "    cluster_teams = cluster_teams.sort_values(by='Distance to KMEANS Centroid', ascending=True)\n",
    "\n",
    "    print(cluster_teams[OFFENSE_FEATURES].head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c39d68",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63471232",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_input = scaled_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206fddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' K Nearest Neighbors '''\n",
    "## GOAL: estimate ideal parameter values for epsilon (eps)\n",
    "\n",
    "# Init model\n",
    "k_neighbors_model = NearestNeighbors()\n",
    "\n",
    "# Fit and find nearest neighbors\n",
    "k_neighbors_model.fit(dbscan_input)\n",
    "distances, indices = k_neighbors_model.kneighbors(dbscan_input)\n",
    "\n",
    "# Plot sorted distances, find the \"elbow\"\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "\n",
    "fig = px.line(\n",
    "    data_frame=distances\n",
    ")\n",
    "fig.update_layout(xaxis_title='Number of SKUs', yaxis_title='Distances (EPS)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Run DBSCAN '''\n",
    "\n",
    "# Params\n",
    "eps_options = [i / 10.0 for i in range(40, 50, 1)]              # Based on above elbow, trying 1.2 to 1.5\n",
    "# min_samples_options = [i for i in range(2, (PCA_N_COMPONENTS*2) + 1, 1)]\n",
    "min_samples_options = [5]\n",
    "\n",
    "# Lists\n",
    "models = []\n",
    "ss_scores = []\n",
    "davies_scores = []\n",
    "\n",
    "for eps in eps_options:\n",
    "    for min_samples in min_samples_options:\n",
    "        # Create model\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "        # Fit\n",
    "        print(f'Fitting')\n",
    "        dbscan_model.fit(dbscan_input)\n",
    "        labels = dbscan_model.labels_\n",
    "\n",
    "        print(f'Processing')\n",
    "        # Number of clusters, ignoring noise\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "        ## Score\n",
    "        non_noise_idxs = np.where(labels != -1)[0]\n",
    "        df_scaled_no_noise = dbscan_input.loc[non_noise_idxs, :]\n",
    "        labels_no_noise = labels[labels != -1]\n",
    "\n",
    "        print(f'Scoring')\n",
    "        ss_score = 0\n",
    "        davies_score = 0\n",
    "        if len(np.unique(labels_no_noise)) > 1:\n",
    "            print(f'Silhouette')\n",
    "            # Silhouette\n",
    "            ss_score = silhouette_score(df_scaled_no_noise, labels_no_noise)    #, sample_size=int(len(scaled_data_df) * 0.25))\n",
    "\n",
    "            print(f'Davies Bouldin')\n",
    "            # Davies-Bouldin\n",
    "            davies_score = davies_bouldin_score(df_scaled_no_noise, labels_no_noise)\n",
    "        else:\n",
    "            # If there's only 1 cluster, don't report score\n",
    "            ss_scores.append(0)\n",
    "\n",
    "        model = {\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters_,\n",
    "            'n_noise': n_noise_,\n",
    "            'silhouette': ss_score,\n",
    "            'davies': davies_score\n",
    "        }\n",
    "        models.append(model)\n",
    "\n",
    "\n",
    "        # Print results\n",
    "        print(f'------------- Model -------------')\n",
    "        print(f'Eps: {eps}')\n",
    "        print(f'Min Samples: {min_samples}')\n",
    "        print()\n",
    "        print(f'Number of clusters: {n_clusters_}')\n",
    "        print(f'Number of noisy points: {n_noise_}')\n",
    "        print(f\"Silhouette Coefficient: {ss_score:.3f}\")\n",
    "        print(f\"Davies-Bouldin Score: {davies_score:.3f}\")\n",
    "        print()\n",
    "\n",
    "results_df = pd.DataFrame.from_records(data=models)\n",
    "\n",
    "print(f'\\n------------ Top Silhouette Scores ------------')\n",
    "print(results_df.sort_values(by='silhouette', ascending=False).head(10).to_string())\n",
    "\n",
    "print(f'\\n------------ Top Davies Scores ------------')\n",
    "print(results_df.sort_values(by='davies', ascending=True).head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Final DBSCAN '''\n",
    "\n",
    "OPTIMAL_EPS = 4.7 #0.58\n",
    "OPTIMAL_MIN_SAMPLES = 5 #9\n",
    "\n",
    "# Create model\n",
    "dbscan_model_final = DBSCAN(eps=OPTIMAL_EPS, min_samples=OPTIMAL_MIN_SAMPLES)\n",
    "\n",
    "# Fit\n",
    "dbscan_model_final.fit(dbscan_input)\n",
    "labels = dbscan_model_final.labels_\n",
    "\n",
    "# Number of clusters, ignoring noise\n",
    "N_CLUSTERS_FINAL = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "N_NOISE_FINAL = list(labels).count(-1)\n",
    "\n",
    "print(f'Number of clusters: {N_CLUSTERS_FINAL}')\n",
    "print(f'Number of noisy points: {N_NOISE_FINAL}')\n",
    "\n",
    "## Add clusters to original dataframe\n",
    "offense_tendencies['Cluster DBSCAN'] = labels\n",
    "offense_tendencies['Cluster DBSCAN'] = offense_tendencies['Cluster DBSCAN'].astype(str)\n",
    "\n",
    "print(offense_tendencies['Cluster DBSCAN'].value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ede3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualize DBSCAN - 3D PCA '''\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=offense_tendencies,\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    z='Component 3',\n",
    "    title='DBSCAN Clusters',\n",
    "    color='Cluster DBSCAN',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(offense_tendencies.loc[offense_tendencies['Cluster DBSCAN'] == '1',:].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc44ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Viz Clusters '''\n",
    "\n",
    "\n",
    "for cluster in range(0, N_CLUSTERS_FINAL):\n",
    "    visualize_cluster(cluster=cluster, alg_name='DBSCAN')\n",
    "\n",
    "    cluster_teams = offense_tendencies.loc[offense_tendencies['Cluster DBSCAN'] == str(cluster),:]\n",
    "    # cluster_teams = cluster_teams.sort_values(by='Distance to KMEANS Centroid', ascending=True)\n",
    "\n",
    "    print(cluster_teams[OFFENSE_FEATURES].head().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
